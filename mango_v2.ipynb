{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#E12E2E\">評分標準</font>\n",
    "## weighted average recall\n",
    "## $\\sum\\limits_{i=1}^NW(i)*Recall(i)$  \n",
    "## 測試集 : 包含決賽、初賽資料 5:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img width=\"800\" height=\"700\" src=\"https://i.imgur.com/ZxOSrFf.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">EDA</font>\n",
    "[EDA_cum_image_processing](https://www.kaggle.com/fireheart7/eda-cum-image-processing#Analysing-the-color-channel-distribution-in-each-Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">去影像模糊、雜訊</font>\n",
    "[Effects of Image Degradation and Degradation\n",
    "Removal to CNN-based Image Classification](https://drive.google.com/file/d/1YmMfXyK19hL0UAkc4l7mKYS2BmMLxDh8/view?usp=sharing)  \n",
    "[Effects of Image Degradations to CNN-based Image\n",
    "Classification](https://arxiv.org/pdf/1810.05552.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img width=\"700\" height=\"700\" src=\"https://i.imgur.com/xK893nn.jpg\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img width=\"700\" height=\"500\" src=\"https://i.imgur.com/6lN8lqw.jpg\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">不平衡數據</font>\n",
    "\n",
    "|    Grade   | Train| Dev|  Test   | \n",
    "| :----- | ----: | :----: | :----: |       \n",
    "| A  | 28585 | 5388| blinded|    \n",
    "| B  | 11140 | 1016| blinded| \n",
    "| C  | 5275 | 596| blinded|   \n",
    "\n",
    "\n",
    "[Focal Loss for Class Imbalance Problem](https://arxiv.org/pdf/2001.03329.pdf)    \n",
    "[不平衡數據處理](https://blog.csdn.net/asialee_bird/article/details/83714612)  \n",
    "[處理imbalanced dataset](https://medium.com/analytics-vidhya/handling-imbalanced-dataset-in-image-classification-dc6f1e13aeee)  \n",
    "[How to deal with Unbalanced Image Datasets in less than 20 lines of code](https://medium.com/analytics-vidhya/how-to-apply-data-augmentation-to-deal-with-unbalanced-datasets-in-20-lines-of-code-ada8521320c9)  \n",
    "[In classification, how do you handle an unbalanced training set?](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set/answers/1144228?srid=h3G6o)  \n",
    "[scikit-learn API:imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)  \n",
    "[Deep learning unbalanced training data?Solve it like this.](https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6)  \n",
    "[A systematic study of the class imbalance problem in convolutional neural networks∗](https://arxiv.org/pdf/1710.05381.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">訓練技巧</font>\n",
    "[label smoothing](https://zhuanlan.zhihu.com/p/101553787)   \n",
    "[test time augmentation(TTA)](https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it)  \n",
    "[Ranger 優化器](https://zhuanlan.zhihu.com/p/100877314)  \n",
    "[SGDR](https://zhuanlan.zhihu.com/p/52084949)  \n",
    "[Mish 激活函數](https://zhuanlan.zhihu.com/p/84418420)  \n",
    "[flat cosine anneal](https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c)  \n",
    "[mixup](https://blog.csdn.net/u013841196/article/details/81049968)  \n",
    "[label smoothing](https://arxiv.org/pdf/1906.02629.pdf)  \n",
    "[transfer learning](https://blog.csdn.net/Emma_Love/article/details/88093975)  \n",
    "[weight decay](https://www.zhihu.com/question/65626362/answer/960145051)  \n",
    "[Snapshot Ensembles](https://zhuanlan.zhihu.com/p/93648558)  \n",
    "[Pseudo Labeling](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969)    \n",
    "[to_fp16](https://zhuanlan.zhihu.com/p/84219777)     \n",
    "[class weight](https://blog.csdn.net/xpy870663266/article/details/104600054)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">調參</font>\n",
    "\n",
    "[Bayesian optimization](https://medium.com/@hiepnguyen034/improving-neural-networks-performance-with-bayesian-optimization-efbaa801ad26)  \n",
    "[調參](https://medium.com/@jacky308082/%E8%87%AA%E5%8B%95%E5%8C%96%E8%AA%BF%E6%95%B4%E8%B6%85%E5%8F%83%E6%95%B8%E6%96%B9%E6%B3%95%E4%BB%8B%E7%B4%B9-%E4%BD%BF%E7%94%A8python-40edb9f0b462)  \n",
    "[貝葉斯優化](https://zhuanlan.zhihu.com/p/53826787)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">模型</font>\n",
    "\n",
    "[ImageNet benchmark](https://paperswithcode.com/sota/image-classification-on-imagenet)  \n",
    "[Fine-Grained Image Classification Benchmarks](https://paperswithcode.com/task/fine-grained-image-classification)  \n",
    "[Pytorch 預訓練模型](https://pytorch.org/hub/research-models)  \n",
    "[ResNeXt](https://zhuanlan.zhihu.com/p/32913695)      \n",
    "[Ghostnet](https://zhuanlan.zhihu.com/p/109325275)    \n",
    "[xResNet](https://towardsdatascience.com/xresnet-from-scratch-in-pytorch-e64e309af722)  \n",
    "[senet](https://arxiv.org/pdf/1709.01507.pdf)  \n",
    "[densenet](https://zhuanlan.zhihu.com/p/37189203)      \n",
    "[effientnet](https://arxiv.org/pdf/1905.11946.pdf)     \n",
    "[resnest](https://zhuanlan.zhihu.com/p/132655457)  \n",
    "[resnet](https://zhuanlan.zhihu.com/p/31852747)  \n",
    "[Pnasnet5large](https://zhuanlan.zhihu.com/p/52798148)   \n",
    "[SqueezeNet、MobileNet、ShuffleNet、Xception](https://zhuanlan.zhihu.com/p/32746221)  \n",
    "[Inception系列](https://zhuanlan.zhihu.com/p/37505777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">細粒度模型</font>\n",
    "\n",
    "[S3N](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ding_Selective_Sparse_Sampling_for_Fine-Grained_Image_Recognition_ICCV_2019_paper.pdf)  \n",
    "[S3N github](https://github.com/Yao-DD/S3N)   \n",
    "[Multi-branch and Multi-scale Attention\n",
    "Learning for Fine-Grained Visual Categorization](https://arxiv.org/pdf/2003.09150v3.pdf)  \n",
    "[Weakly Supervised Fine-grained Image Classification via Guassian Mixture\n",
    "Model Oriented Discriminative Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Weakly_Supervised_Fine-Grained_Image_Classification_via_Guassian_Mixture_Model_Oriented_CVPR_2020_paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">切割圖片</font>\n",
    "\n",
    "[Mask_RCNN](https://github.com/matterport/Mask_RCNN)    \n",
    "[Instance Segmentation with Mask R-CNN](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">模型融合</font>\n",
    "[模型融合](https://zhuanlan.zhihu.com/p/25836678)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">比賽</font>\n",
    "[kaggle 亞馬遜雨林衛星圖像比賽 1st](https://zhuanlan.zhihu.com/p/28084438)  \n",
    "[Kaggle Top 2% APTOS 2019](https://zhuanlan.zhihu.com/p/81695773)    \n",
    "[Kaggle 識別座頭鯨 1st solution](https://zhuanlan.zhihu.com/p/58496385)  \n",
    "[Kaggle 識別座頭鯨 code](https://github.com/earhian/Humpback-Whale-Identification-1st-)    \n",
    "[Kaggle 分辨雜草和植物幼苗 5th solution](https://zhuanlan.zhihu.com/p/38359300)  \n",
    "[1st place Solution for Intel Scene Classification Challenge](https://towardsdatascience.com/1st-place-solution-for-intel-scene-classification-challenge-c95cf941f8ed)  \n",
    "[2019細粒度圖像分類挑戰賽冠軍](https://www.zhihu.com/question/331320468/answer/727015760)   \n",
    "[第二屆腫瘤切割挑戰賽-第二名](https://medium.com/@aaronkao/%E8%A4%87%E7%9B%A4-%E7%AC%AC%E4%BA%8C%E5%B1%86%E5%8F%B0%E7%81%A3%E8%85%AB%E7%98%A4%E5%88%87%E5%89%B2%E6%8C%91%E6%88%B0%E8%B3%BD-6b3742cdb5cb)  \n",
    "## kaggle fgvc7\n",
    "[kaggle FGVC競賽](https://www.kaggle.com/search?q=FGVC+in%3Acompetitions)  \n",
    "## kaggle fgvc7\n",
    "[kaggle FGVC競賽](https://www.kaggle.com/search?q=FGVC+in%3Acompetitions)  \n",
    "### Semi-Supervised Recognition Challenge - FGVC7\n",
    "[3rd place solution](https://arxiv.org/pdf/2006.10702.pdf)  \n",
    "[1st Place Solution](https://www.kaggle.com/c/semi-inat-2020/discussion/160724)\n",
    "### iWildCam 2020\n",
    "[3rd place solution](https://www.kaggle.com/c/iwildcam-2020-fgvc7/discussion/157932)\n",
    "### Herbarium 2020\n",
    "[1st place solution](https://www.kaggle.com/c/herbarium-2020-fgvc7/discussion/154351)  \n",
    "[2nd Place Solution](https://www.kaggle.com/c/herbarium-2020-fgvc7/discussion/154186)\n",
    "### iMaterialist Challenge (Furniture) at FGVC5\n",
    "[4th place solution](https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/discussion/57939)\n",
    "### iMet Collection 2019 - FGVC6\n",
    "[2nd place solution](https://www.kaggle.com/c/imet-2019-fgvc6/discussion/96149)  \n",
    "[1st place solution](https://www.kaggle.com/c/imet-2019-fgvc6/discussion/94687)\n",
    "### Plant Pathology 2020\n",
    "[2020植物病理分類1st方案](https://github.com/alipay/cvpr2020-plant-pathology)   \n",
    "[BiLinear EfficientNet Focal Loss+ Label Smoothing (20th)](https://www.kaggle.com/jimitshah777/bilinear-efficientnet-focal-loss-label-smoothing)  \n",
    "[2nd place Solution](https://www.kaggle.com/c/plant-pathology-2020-fgvc7/discussion/155929)  \n",
    "[leaf_disease_main_notebook](https://www.kaggle.com/fireheart7/leaf-disease-main-notebook/notebook)  \n",
    "[Plant Pathology 2020 in PyTorch - 0.974 score](https://www.kaggle.com/akasharidas/plant-pathology-2020-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">參考</font>\n",
    "[多個模型](http://home.ifi.uio.no/paalh/publications/files/ism2018-ovr.pdf)  \n",
    "[Image Classification Baseline Model For 2020](https://towardsdatascience.com/image-classification-baseline-model-for-2020-1d33f0986fc0)  \n",
    "[How we beat the FastAI leaderboard score by +19.77%](https://medium.com/@lessw/how-we-beat-the-fastai-leaderboard-score-by-19-77-a-cbb2338fab5c)  \n",
    "[Mango Classification kaggle範例](https://www.kaggle.com/rkuo2000/mango-classification)  \n",
    "[fastai 技巧](https://zhuanlan.zhihu.com/p/41192499)   \n",
    "[Facial age prediction with Fastai2](https://medium.com/analytics-vidhya/facial-age-prediction-with-fastai2-d67fdb575539)  \n",
    "[Senet詳解](https://cloud.tencent.com/developer/article/1610426)  \n",
    "[label smoothing理解](https://zhuanlan.zhihu.com/p/72685158)  \n",
    "[focal loss理解](https://zhuanlan.zhihu.com/p/80594704)   \n",
    "[cs230-deep learning](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks#good-practices)  \n",
    "[fastai技巧](https://zhuanlan.zhihu.com/p/41379279)  \n",
    "[lr_find()](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html)  \n",
    "[小白通过kaggle学习图像分类笔记之二](https://zhuanlan.zhihu.com/p/105190491)    \n",
    "[小白通過kaggle學習圖像分類筆記](https://zhuanlan.zhihu.com/p/104694474)  \n",
    "[fastai-callbacks](https://medium.com/@lessw/fastais-callbacks-for-better-cnn-training-meet-savemodelcallback-e55f254f1af5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#2513A9\">GAN</font>\n",
    "[SinGAN](https://arxiv.org/pdf/1905.01164.pdf)  \n",
    "[WGAN](http://ceur-ws.org/Vol-2563/aics_34.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai2\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from fastai.vision.models.cadene_models import *\n",
    "from fastai2.callback.schedule import fit_flat_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.vision.learner import cnn_config\n",
    "from fastai.callbacks import *\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import shutil\n",
    "import math\n",
    "from math import floor\n",
    "from fastai2.test_utils import *\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設置資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 處理數據不平衡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去影像模糊、雜訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切割圖片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram equalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Focal loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranger optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "class Ranger(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3,                       # lr\n",
    "                 alpha=0.5, k=6, N_sma_threshhold=5,           # Ranger options\n",
    "                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
    "                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
    "                 use_gc=True, gc_conv_only=False\n",
    "                 ):\n",
    "\n",
    "        # parameter checks\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        if not lr > 0:\n",
    "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
    "        if not eps > 0:\n",
    "            raise ValueError(f'Invalid eps: {eps}')\n",
    "\n",
    "        # parameter comments:\n",
    "        # beta1 (momentum) of .95 seems to work better than .90...\n",
    "        # N_sma_threshold of 5 seems better in testing than 4.\n",
    "        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
    "\n",
    "        # prep defaults and init torch.optim base\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
    "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "        # adjustable threshold\n",
    "        self.N_sma_threshhold = N_sma_threshhold\n",
    "\n",
    "        # look ahead params\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "        # radam buffer for state\n",
    "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
    "\n",
    "        # gc on or off\n",
    "        self.use_gc = use_gc\n",
    "\n",
    "        # level of gradient centralization\n",
    "        self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
    "\n",
    "        print(\n",
    "            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n",
    "        if (self.use_gc and self.gc_gradient_threshold == 1):\n",
    "            print(f\"GC applied to both conv and fc layers\")\n",
    "        elif (self.use_gc and self.gc_gradient_threshold == 3):\n",
    "            print(f\"GC applied to conv layers only\")\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        print(\"set state called\")\n",
    "        super(Ranger, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n",
    "        # Uncomment if you need to use the actual closure...\n",
    "\n",
    "        # if closure is not None:\n",
    "        #loss = closure()\n",
    "\n",
    "        # Evaluate averages and grad, update param tensors\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\n",
    "                        'Ranger optimizer does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]  # get state dict for this param\n",
    "\n",
    "                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
    "                    # if self.first_run_check==0:\n",
    "                    # self.first_run_check=1\n",
    "                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "\n",
    "                    # look ahead weight storage now in state dict\n",
    "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
    "                    state['slow_buffer'].copy_(p.data)\n",
    "\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
    "                        p_data_fp32)\n",
    "\n",
    "                # begin computations\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # GC operation for Conv layers and FC layers\n",
    "                if grad.dim() > self.gc_gradient_threshold:\n",
    "                    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # compute variance mov avg\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                # compute mean moving avg\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * \\\n",
    "                        state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    if N_sma > self.N_sma_threshhold:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
    "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay']\n",
    "                                     * group['lr'], p_data_fp32)\n",
    "\n",
    "                # apply lr\n",
    "                if N_sma > self.N_sma_threshhold:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size *\n",
    "                                         group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "                # integrated look ahead...\n",
    "                # we do it at the param level instead of group level\n",
    "                if state['step'] % group['k'] == 0:\n",
    "                    # get access to slow param tensor\n",
    "                    slow_p = state['slow_buffer']\n",
    "                    # (fast weights - slow weights) * alpha\n",
    "                    slow_p.add_(self.alpha, p.data - slow_p)\n",
    "                    # copy interpolated weights to RAdam param tensor\n",
    "                    p.data.copy_(slow_p)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "<hr style=\"height:5px;background-color:red\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW  one-cycle-policy\n",
    "# SGD    sgdr\n",
    "# Ranger flat cosine annealing \n",
    "# Mixup\n",
    "# Labelsmoothing\n",
    "# train 30 epochs\n",
    "# fine tune 20 epochs (lr、wd 調整)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# senet154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranger=partial(Ranger)\n",
    "learn = cnn_learner(data,arch,metrics=[accuracy,Recall('weighted'),FBeta('macro')],opt_func=ranger,loss_func=LabelSmoothingCrossEntropy(),pretrained=True).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_fc(learn,30,1e-02,wd=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test time augmentation\n",
    "preds,targs = learn.TTA()\n",
    "accuracy(preds, targs).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# se_resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# se_resnext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=EfficientNet.from_pretrained('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = model._fc.in_features\n",
    "model._fc = nn.Linear(in_features=feature,out_features=3,bias=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._fc.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.kaiming_uniform_(model._fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct=0\n",
    "for child in learn.model.children():\n",
    "    ct+=1\n",
    "    if ct<2:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pnasnet5large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inceptionresnetv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dpn92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNext(WSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNeSt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/zhanghang1989/ResNeSt/archive/master.zip\" to /home/aistudent/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['resnest101',\n",
       " 'resnest200',\n",
       " 'resnest269',\n",
       " 'resnest50',\n",
       " 'resnest50_fast_1s1x64d',\n",
       " 'resnest50_fast_1s2x40d',\n",
       " 'resnest50_fast_1s4x24d',\n",
       " 'resnest50_fast_2s1x64d',\n",
       " 'resnest50_fast_2s2x40d',\n",
       " 'resnest50_fast_4s1x64d',\n",
       " 'resnest50_fast_4s2x40d']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hub.list('zhanghang1989/ResNeSt', force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest101', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(data,arch,metrics=[accuracy,Recall('weighted'),FBeta('macro')],opt_func=ranger,loss_func=LabelSmoothingCrossEntropy(),pretrained=True).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in learn.layer_groups[0]:\n",
    "    if type(layer)!=nn.Linear and type(layer)!=nn.BatchNorm2d:\n",
    "        for param in layer.parameters():\n",
    "                param.requires_grad = False       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xception_cadene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nasnetamobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inceptionv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GhostNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vgg-nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# squeezenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mobilenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shufflenet\n",
    "<hr style=\"height:5px;background-color:red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 細粒度模型\n",
    "<hr style=\"height:5px;background-color:blue\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;background-color:blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mxresnet with ranger(非預訓練模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "%run mxresnet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranger=partial(Ranger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch=mxresnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relu_to_mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            convert_relu_to_mish(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_relu_to_mish(arch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data,arch,metrics=[accuracy,Recall('weighted'),FBeta('macro'],bn_wd=False,true_wd=True,wd=1e-02,opt_func=ranger,loss_func=LabelSmoothingCrossEntropy()).to_fp16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 雙模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search , Bayesian optimization(調參)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 移除 cv>0.97 分類錯誤圖片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain\n",
    "### &emsp;  <font color='#FF7133'>1. 5-fold cv</font>\n",
    "### &emsp; <font color='#FF7133'>2. Random search or Bayesian optimization</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測test data、製作csv\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<hr style=\"height:5px;background-color:purple\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#5B2C6F>fastai.vision learner.py</font>\n",
    "### https://github.com/fastai/fastai/blob/master/fastai/vision/learner.py#L94\n",
    "\n",
    "```python\n",
    "\"`Learner` support for computer vision\"\n",
    "from ..torch_core import *\n",
    "from ..basic_train import *\n",
    "from ..basic_data import *\n",
    "from .image import *\n",
    "from . import models\n",
    "from ..callback import *\n",
    "from ..layers import *\n",
    "from ..callbacks.hooks import *\n",
    "from ..train import ClassificationInterpretation\n",
    "\n",
    "__all__ = ['cnn_learner', 'create_cnn', 'create_cnn_model', 'create_body', 'create_head', 'unet_learner']\n",
    "# By default split models between first and second layer\n",
    "def _default_split(m:nn.Module): return (m[1],)\n",
    "# Split a resnet style model\n",
    "def _resnet_split(m:nn.Module): return (m[0][6],m[1])\n",
    "# Split squeezenet model on maxpool layers\n",
    "def _squeezenet_split(m:nn.Module): return (m[0][0][5], m[0][0][8], m[1])\n",
    "def _densenet_split(m:nn.Module): return (m[0][0][7],m[1])\n",
    "def _vgg_split(m:nn.Module): return (m[0][0][22],m[1])\n",
    "def _alexnet_split(m:nn.Module): return (m[0][0][6],m[1])\n",
    "def _mobilenetv2_split(m:nn.Module): return (m[0][0][10],m[1])\n",
    "\n",
    "_default_meta     = {'cut':None, 'split':_default_split}\n",
    "_resnet_meta      = {'cut':-2, 'split':_resnet_split }\n",
    "_squeezenet_meta  = {'cut':-1, 'split': _squeezenet_split}\n",
    "_densenet_meta    = {'cut':-1, 'split':_densenet_split}\n",
    "_vgg_meta         = {'cut':-1, 'split':_vgg_split}\n",
    "_alexnet_meta     = {'cut':-1, 'split':_alexnet_split}\n",
    "_mobilenetv2_meta = {'cut':-1, 'split':_mobilenetv2_split}\n",
    "\n",
    "model_meta = {\n",
    "    models.resnet18 :{**_resnet_meta}, models.resnet34: {**_resnet_meta},\n",
    "    models.resnet50 :{**_resnet_meta}, models.resnet101:{**_resnet_meta},\n",
    "    models.resnet152:{**_resnet_meta},\n",
    "\n",
    "    models.squeezenet1_0:{**_squeezenet_meta},\n",
    "    models.squeezenet1_1:{**_squeezenet_meta},\n",
    "\n",
    "    models.densenet121:{**_densenet_meta}, models.densenet169:{**_densenet_meta},\n",
    "    models.densenet201:{**_densenet_meta}, models.densenet161:{**_densenet_meta},\n",
    "    models.vgg11_bn:{**_vgg_meta}, models.vgg13_bn:{**_vgg_meta}, models.vgg16_bn:{**_vgg_meta}, models.vgg19_bn:{**_vgg_meta},\n",
    "    models.alexnet:{**_alexnet_meta},\n",
    "    models.mobilenet_v2:{**_mobilenetv2_meta}}\n",
    "\n",
    "def cnn_config(arch):\n",
    "    \"Get the metadata associated with `arch`.\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    return model_meta.get(arch, _default_meta)\n",
    "\n",
    "def has_pool_type(m):\n",
    "    if is_pool_type(m): return True\n",
    "    for l in m.children():\n",
    "        if has_pool_type(l): return True\n",
    "    return False\n",
    "\n",
    "def create_body(arch:Callable, pretrained:bool=True, cut:Optional[Union[int, Callable]]=None):\n",
    "    \"Cut off the body of a typically pretrained `model` at `cut` (int) or cut the model as specified by `cut(model)` (function).\"\n",
    "    model = arch(pretrained)\n",
    "    cut = ifnone(cut, cnn_config(arch)['cut'])\n",
    "    if cut is None:\n",
    "        ll = list(enumerate(model.children()))\n",
    "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
    "    if   isinstance(cut, int):      return nn.Sequential(*list(model.children())[:cut])\n",
    "    elif isinstance(cut, Callable): return cut(model)\n",
    "    else:                           raise NamedError(\"cut must be either integer or a function\")\n",
    "\n",
    "\n",
    "def create_head(nf:int, nc:int, lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5,\n",
    "                concat_pool:bool=True, bn_final:bool=False):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.\"\n",
    "    lin_ftrs = [nf, 512, nc] if lin_ftrs is None else [nf] + lin_ftrs + [nc]\n",
    "    ps = listify(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n",
    "    layers = [pool, Flatten()]\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += bn_drop_lin(ni, no, True, p, actn)\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def create_cnn_model(base_arch:Callable, nc:int, cut:Union[int,Callable]=None, pretrained:bool=True,\n",
    "                     lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5, custom_head:Optional[nn.Module]=None,\n",
    "                     bn_final:bool=False, concat_pool:bool=True):\n",
    "    \"Create custom convnet architecture\"\n",
    "    body = create_body(base_arch, pretrained, cut)\n",
    "    if custom_head is None:\n",
    "        nf = num_features_model(nn.Sequential(*body.children())) * (2 if concat_pool else 1)\n",
    "        head = create_head(nf, nc, lin_ftrs, ps=ps, concat_pool=concat_pool, bn_final=bn_final)\n",
    "    else: head = custom_head\n",
    "    return nn.Sequential(body, head)\n",
    "\n",
    "def cnn_learner(data:DataBunch, base_arch:Callable, cut:Union[int,Callable]=None, pretrained:bool=True,\n",
    "                lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5, custom_head:Optional[nn.Module]=None,\n",
    "                split_on:Optional[SplitFuncOrIdxList]=None, bn_final:bool=False, init=nn.init.kaiming_normal_,\n",
    "                concat_pool:bool=True, **kwargs:Any)->Learner:\n",
    "    \"Build convnet style learner.\"\n",
    "    meta = cnn_config(base_arch)\n",
    "    model = create_cnn_model(base_arch, data.c, cut, pretrained, lin_ftrs, ps=ps, custom_head=custom_head,\n",
    "        bn_final=bn_final, concat_pool=concat_pool)\n",
    "    learn = Learner(data, model, **kwargs)\n",
    "    learn.split(split_on or meta['split'])\n",
    "    if pretrained: learn.freeze()\n",
    "    if init: apply_init(model[1], init)\n",
    "    return learn\n",
    "\n",
    "def create_cnn(data, base_arch, **kwargs):\n",
    "    warn(\"`create_cnn` is deprecated and is now named `cnn_learner`.\")\n",
    "    return cnn_learner(data, base_arch, **kwargs)\n",
    "\n",
    "def unet_learner(data:DataBunch, arch:Callable, pretrained:bool=True, blur_final:bool=True,\n",
    "                 norm_type:Optional[NormType]=None, split_on:Optional[SplitFuncOrIdxList]=None, blur:bool=False,\n",
    "                 self_attention:bool=False, y_range:Optional[Tuple[float,float]]=None, last_cross:bool=True,\n",
    "                 bottle:bool=False, cut:Union[int,Callable]=None, **learn_kwargs:Any)->Learner:\n",
    "    \"Build Unet learner from `data` and `arch`.\"\n",
    "    meta = cnn_config(arch)\n",
    "    body = create_body(arch, pretrained, cut)\n",
    "    try:    size = data.train_ds[0][0].size\n",
    "    except: size = next(iter(data.train_dl))[0].shape[-2:]\n",
    "    model = to_device(models.unet.DynamicUnet(body, n_classes=data.c, img_size=size, blur=blur, blur_final=blur_final,\n",
    "          self_attention=self_attention, y_range=y_range, norm_type=norm_type, last_cross=last_cross,\n",
    "          bottle=bottle), data.device)\n",
    "    learn = Learner(data, model, **learn_kwargs)\n",
    "    learn.split(ifnone(split_on, meta['split']))\n",
    "    if pretrained: learn.freeze()\n",
    "    apply_init(model[2], nn.init.kaiming_normal_)\n",
    "    return learn\n",
    "\n",
    "@classmethod\n",
    "def _cl_int_from_learner(cls, learn:Learner, ds_type:DatasetType=DatasetType.Valid, activ:nn.Module=None, tta=False):\n",
    "    \"Create an instance of `ClassificationInterpretation`. `tta` indicates if we want to use Test Time Augmentation.\"\n",
    "    preds = learn.TTA(ds_type=ds_type, with_loss=True) if tta else learn.get_preds(ds_type=ds_type, activ=activ, with_loss=True)\n",
    "\n",
    "    return cls(learn, *preds, ds_type=ds_type)\n",
    "\n",
    "def _test_cnn(m):\n",
    "    if not isinstance(m, nn.Sequential) or not len(m) == 2: return False\n",
    "    return isinstance(m[1][0], (AdaptiveConcatPool2d, nn.AdaptiveAvgPool2d))\n",
    "\n",
    "def _cl_int_gradcam(self, idx, ds_type:DatasetType=DatasetType.Valid, heatmap_thresh:int=16, image:bool=True):\n",
    "    m = self.learn.model.eval()\n",
    "    im,cl = self.learn.data.dl(ds_type).dataset[idx]\n",
    "    cl = int(cl)\n",
    "    xb,_ = self.data.one_item(im, detach=False, denorm=False) #put into a minibatch of batch size = 1\n",
    "    with hook_output(m[0]) as hook_a:\n",
    "        with hook_output(m[0], grad=True) as hook_g:\n",
    "            preds = m(xb)\n",
    "            preds[0,int(cl)].backward()\n",
    "    acts  = hook_a.stored[0].cpu() #activation maps\n",
    "    if (acts.shape[-1]*acts.shape[-2]) >= heatmap_thresh:\n",
    "        grad = hook_g.stored[0][0].cpu()\n",
    "        grad_chan = grad.mean(1).mean(1)\n",
    "        mult = F.relu(((acts*grad_chan[...,None,None])).sum(0))\n",
    "        if image:\n",
    "            xb_im = Image(xb[0])\n",
    "            _,ax = plt.subplots()\n",
    "            sz = list(xb_im.shape[-2:])\n",
    "            xb_im.show(ax,title=f\"pred. class: {self.pred_class[idx]}, actual class: {self.learn.data.classes[cl]}\")\n",
    "            ax.imshow(mult, alpha=0.4, extent=(0,*sz[::-1],0),\n",
    "              interpolation='bilinear', cmap='magma')\n",
    "        return mult\n",
    "\n",
    "ClassificationInterpretation.GradCAM =_cl_int_gradcam\n",
    "\n",
    "def _cl_int_plot_top_losses(self, k, largest=True, figsize=(12,12), heatmap:bool=False, heatmap_thresh:int=16,\n",
    "                            alpha:float=0.6, cmap:str=\"magma\", show_text:bool=True,\n",
    "                            return_fig:bool=None)->Optional[plt.Figure]:\n",
    "    \"Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.\"\n",
    "    assert not heatmap or _test_cnn(self.learn.model), \"`heatmap=True` requires a model like `cnn_learner` produces.\"\n",
    "    if heatmap is None: heatmap = _test_cnn(self.learn.model)\n",
    "    tl_val,tl_idx = self.top_losses(k, largest)\n",
    "    classes = self.data.classes\n",
    "    cols = math.ceil(math.sqrt(k))\n",
    "    rows = math.ceil(k/cols)\n",
    "    fig,axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if show_text: fig.suptitle('Prediction/Actual/Loss/Probability', weight='bold', size=14)\n",
    "    for i,idx in enumerate(tl_idx):\n",
    "        im,cl = self.data.dl(self.ds_type).dataset[idx]\n",
    "        cl = int(cl)\n",
    "        title = f'{classes[self.pred_class[idx]]}/{classes[cl]} / {self.losses[idx]:.2f} / {self.preds[idx][cl]:.2f}' if show_text else None\n",
    "        im.show(ax=axes.flat[i], title=title)\n",
    "        if heatmap:\n",
    "            mult = self.GradCAM(idx,self.ds_type,heatmap_thresh,image=False)\n",
    "            if mult is not None:\n",
    "                sz = list(im.shape[-2:])\n",
    "                axes.flat[i].imshow(mult, alpha=alpha, extent=(0,*sz[::-1],0), interpolation='bilinear', cmap=cmap)\n",
    "    if ifnone(return_fig, defaults.return_fig): return fig\n",
    "\n",
    "def _cl_int_plot_multi_top_losses(self, samples:int=3, figsize:Tuple[int,int]=(8,8), save_misclassified:bool=False):\n",
    "    \"Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class in a multilabeled dataset.\"\n",
    "    if samples >20:\n",
    "        print(\"Max 20 samples\")\n",
    "        return\n",
    "    losses, idxs = self.top_losses(self.data.c)\n",
    "    l_dim = len(losses.size())\n",
    "    if l_dim == 1: losses, idxs = self.top_losses()\n",
    "    infolist, ordlosses_idxs, mismatches_idxs, mismatches, losses_mismatches, mismatchescontainer = [],[],[],[],[],[]\n",
    "    truthlabels = np.asarray(self.y_true, dtype=int)\n",
    "    classes_ids = [k for k in enumerate(self.data.classes)]\n",
    "    predclass = np.asarray(self.pred_class)\n",
    "    for i,pred in enumerate(predclass):\n",
    "        where_truth = np.nonzero((truthlabels[i]>0))[0]\n",
    "        mismatch = np.all(pred!=where_truth)\n",
    "        if mismatch:\n",
    "            mismatches_idxs.append(i)\n",
    "            if l_dim > 1 : losses_mismatches.append((losses[i][pred], i))\n",
    "            else: losses_mismatches.append((losses[i], i))\n",
    "        if l_dim > 1: infotup = (i, pred, where_truth, losses[i][pred], np.round(self.preds[i], decimals=3)[pred], mismatch)\n",
    "        else: infotup = (i, pred, where_truth, losses[i], np.round(self.preds[i], decimals=3)[pred], mismatch)\n",
    "        infolist.append(infotup)\n",
    "    ds = self.data.dl(self.ds_type).dataset\n",
    "    mismatches = ds[mismatches_idxs]\n",
    "    ordlosses = sorted(losses_mismatches, key = lambda x: x[0], reverse=True)\n",
    "    for w in ordlosses: ordlosses_idxs.append(w[1])\n",
    "    mismatches_ordered_byloss = ds[ordlosses_idxs]\n",
    "    print(f'{str(len(mismatches))} misclassified samples over {str(len(self.data.valid_ds))} samples in the validation set.')\n",
    "    samples = min(samples, len(mismatches))\n",
    "    for ima in range(len(mismatches_ordered_byloss)):\n",
    "        mismatchescontainer.append(mismatches_ordered_byloss[ima][0])\n",
    "    for sampleN in range(samples):\n",
    "        actualclasses = ''\n",
    "        for clas in infolist[ordlosses_idxs[sampleN]][2]:\n",
    "            actualclasses = f'{actualclasses} -- {str(classes_ids[clas][1])}'\n",
    "        imag = mismatches_ordered_byloss[sampleN][0]\n",
    "        imag = show_image(imag, figsize=figsize)\n",
    "        imag.set_title(f\"\"\"Predicted: {classes_ids[infolist[ordlosses_idxs[sampleN]][1]][1]} \\nActual: {actualclasses}\\nLoss: {infolist[ordlosses_idxs[sampleN]][3]}\\nProbability: {infolist[ordlosses_idxs[sampleN]][4]}\"\"\",\n",
    "                        loc='left')\n",
    "        plt.show()\n",
    "        if save_misclassified: return mismatchescontainer\n",
    "\n",
    "ClassificationInterpretation.from_learner          = _cl_int_from_learner\n",
    "ClassificationInterpretation.plot_top_losses       = _cl_int_plot_top_losses\n",
    "ClassificationInterpretation.plot_multi_top_losses = _cl_int_plot_multi_top_losses\n",
    "\n",
    "\n",
    "def _learner_interpret(learn:Learner, ds_type:DatasetType=DatasetType.Valid, tta=False):\n",
    "    \"Create a `ClassificationInterpretation` object from `learner` on `ds_type` with `tta`.\"\n",
    "    return ClassificationInterpretation.from_learner(learn, ds_type=ds_type, tta=tta)\n",
    "Learner.interpret = _learner_interpret\n",
    "```\n",
    "<hr style=\"height:5px;background-color:#FFAF33\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#5B2C6F>github fastai2 xresnet.py</font>\n",
    "### https://github.com/fastai/fastai2/blob/master/fastai2/vision/models/xresnet.py\n",
    "\n",
    "```python\n",
    "\n",
    "__all__ = ['init_cnn', 'XResNet', 'xresnet18', 'xresnet34', 'xresnet50', 'xresnet101', 'xresnet152', 'xresnet18_deep',\n",
    "           'xresnet34_deep', 'xresnet50_deep', 'xresnet18_deeper', 'xresnet34_deeper', 'xresnet50_deeper', 'se_kwargs1',\n",
    "           'se_kwargs2', 'se_kwargs3', 'g0', 'g1', 'g2', 'g3', 'xse_resnet18', 'xse_resnext18', 'xresnext18',\n",
    "           'xse_resnet34', 'xse_resnext34', 'xresnext34', 'xse_resnet50', 'xse_resnext50', 'xresnext50',\n",
    "           'xse_resnet101', 'xse_resnext101', 'xresnext101', 'xse_resnet152', 'xsenet154', 'xse_resnext18_deep',\n",
    "           'xse_resnext34_deep', 'xse_resnext50_deep', 'xse_resnext18_deeper', 'xse_resnext34_deeper',\n",
    "           'xse_resnext50_deeper']\n",
    "\n",
    "# Cell\n",
    "from ...torch_basics import *\n",
    "from torchvision.models.utils import load_state_dict_from_url\n",
    "\n",
    "# Cell\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "# Cell\n",
    "class XResNet(nn.Sequential):\n",
    "    @delegates(ResBlock)\n",
    "    def __init__(self, block, expansion, layers, p=0.0, c_in=3, n_out=1000, stem_szs=(32,32,64),\n",
    "                 widen=1.0, sa=False, act_cls=defaults.activation, **kwargs):\n",
    "        store_attr(self, 'block,expansion,act_cls')\n",
    "        stem_szs = [c_in, *stem_szs]\n",
    "        stem = [ConvLayer(stem_szs[i], stem_szs[i+1], stride=2 if i==0 else 1, act_cls=act_cls)\n",
    "                for i in range(3)]\n",
    "\n",
    "        block_szs = [int(o*widen) for o in [64,128,256,512] +[256]*(len(layers)-4)]\n",
    "        block_szs = [64//expansion] + block_szs\n",
    "        blocks    = self._make_blocks(layers, block_szs, sa, **kwargs)\n",
    "\n",
    "        super().__init__(\n",
    "            *stem, nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *blocks,\n",
    "            nn.AdaptiveAvgPool2d(1), Flatten(), nn.Dropout(p),\n",
    "            nn.Linear(block_szs[-1]*expansion, n_out),\n",
    "        )\n",
    "        init_cnn(self)\n",
    "\n",
    "    def _make_blocks(self, layers, block_szs, sa, **kwargs):\n",
    "        return [self._make_layer(ni=block_szs[i], nf=block_szs[i+1], blocks=l,\n",
    "                                 stride=1 if i==0 else 2, sa=sa and i==len(layers)-4, **kwargs)\n",
    "                for i,l in enumerate(layers)]\n",
    "\n",
    "    def _make_layer(self, ni, nf, blocks, stride, sa, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            *[self.block(self.expansion, ni if i==0 else nf, nf, stride=stride if i==0 else 1,\n",
    "                      sa=sa and i==(blocks-1), act_cls=self.act_cls, **kwargs)\n",
    "              for i in range(blocks)])\n",
    "\n",
    "# Cell\n",
    "def _xresnet(pretrained, expansion, layers, **kwargs):\n",
    "    # TODO pretrain all sizes. Currently will fail with non-xrn50\n",
    "    url = 'https://s3.amazonaws.com/fast-ai-modelzoo/xrn50_940.pth'\n",
    "    res = XResNet(ResBlock, expansion, layers, **kwargs)\n",
    "    if pretrained: res.load_state_dict(load_state_dict_from_url(url, map_location='cpu')['model'], strict=False)\n",
    "    return res\n",
    "\n",
    "def xresnet18 (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2, 2,  2, 2], **kwargs)\n",
    "def xresnet34 (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet50 (pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 4,  6, 3], **kwargs)\n",
    "def xresnet101(pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 4, 23, 3], **kwargs)\n",
    "def xresnet152(pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3, 8, 36, 3], **kwargs)\n",
    "def xresnet18_deep  (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2,2,2,2,1,1], **kwargs)\n",
    "def xresnet34_deep  (pretrained=False, **kwargs): return _xresnet(pretrained, 1, [3,4,6,3,1,1], **kwargs)\n",
    "def xresnet50_deep  (pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3,4,6,3,1,1], **kwargs)\n",
    "def xresnet18_deeper(pretrained=False, **kwargs): return _xresnet(pretrained, 1, [2,2,1,1,1,1,1,1], **kwargs)\n",
    "def xresnet34_deeper(pretrained=False, **kwargs): return _xresnet(pretrained, 1, [3,4,6,3,1,1,1,1], **kwargs)\n",
    "def xresnet50_deeper(pretrained=False, **kwargs): return _xresnet(pretrained, 4, [3,4,6,3,1,1,1,1], **kwargs)\n",
    "\n",
    "# Cell\n",
    "se_kwargs1 = dict(groups=1 , reduction=16)\n",
    "se_kwargs2 = dict(groups=32, reduction=16)\n",
    "se_kwargs3 = dict(groups=32, reduction=0)\n",
    "g0 = [2,2,2,2]\n",
    "g1 = [3,4,6,3]\n",
    "g2 = [3,4,23,3]\n",
    "g3 = [3,8,36,3]\n",
    "\n",
    "# Cell\n",
    "def xse_resnet18(n_out=1000, pretrained=False, **kwargs):   return XResNet(SEBlock,  1, g0, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "def xse_resnext18(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g0, n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xresnext18(n_out=1000, pretrained=False, **kwargs):     return XResNet(SEResNeXtBlock, 1, g0, n_out=n_out, **se_kwargs3, **kwargs)\n",
    "def xse_resnet34(n_out=1000, pretrained=False, **kwargs):   return XResNet(SEBlock,  1, g1, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "def xse_resnext34(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g1, n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xresnext34(n_out=1000, pretrained=False, **kwargs):     return XResNet(SEResNeXtBlock, 1, g1, n_out=n_out, **se_kwargs3, **kwargs)\n",
    "def xse_resnet50(n_out=1000, pretrained=False, **kwargs):   return XResNet(SEBlock,  4, g1, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "def xse_resnext50(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 4, g1, n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xresnext50(n_out=1000, pretrained=False, **kwargs):     return XResNet(SEResNeXtBlock, 4, g1, n_out=n_out, **se_kwargs3, **kwargs)\n",
    "def xse_resnet101(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEBlock,  4, g2, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "def xse_resnext101(n_out=1000, pretrained=False, **kwargs): return XResNet(SEResNeXtBlock, 4, g2, n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xresnext101(n_out=1000, pretrained=False, **kwargs):    return XResNet(SEResNeXtBlock, 4, g2, n_out=n_out, **se_kwargs3, **kwargs)\n",
    "def xse_resnet152(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEBlock,  4, g3, n_out=n_out, **se_kwargs1, **kwargs)\n",
    "def xsenet154(n_out=1000, pretrained=False, **kwargs):\n",
    "    return XResNet(SEBlock, g3, groups=64, reduction=16, p=0.2, n_out=n_out)\n",
    "def xse_resnext18_deep  (n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g0+[1,1], n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xse_resnext34_deep  (n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, g1+[1,1], n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xse_resnext50_deep  (n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 4, g1+[1,1], n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xse_resnext18_deeper(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, [2,2,1,1,1,1,1,1], n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xse_resnext34_deeper(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 1, [3,4,4,2,2,1,1,1], n_out=n_out, **se_kwargs2, **kwargs)\n",
    "def xse_resnext50_deeper(n_out=1000, pretrained=False, **kwargs):  return XResNet(SEResNeXtBlock, 4, [3,4,4,2,2,1,1,1], n_out=n_out, **se_kwargs2, **kwargs)\n",
    "```\n",
    "<hr style=\"height:5px;background-color:#FFAF33\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=#5B2C6F>fastai pretrained cadene_models</font> \n",
    "### https://github.com/fastai/fastai/blob/master/fastai/vision/models/cadene_models.py\n",
    "```python\n",
    "#These models are dowloaded via the repo https://github.com/Cadene/pretrained-models.pytorch\n",
    "#See licence here: https://github.com/Cadene/pretrained-models.pytorch/blob/master/LICENSE.txt\n",
    "from torch import nn\n",
    "from ..learner import model_meta\n",
    "from ...core import *\n",
    "\n",
    "pretrainedmodels = try_import('pretrainedmodels')\n",
    "if not pretrainedmodels:\n",
    "    raise Exception('Error: `pretrainedmodels` is needed. `pip install pretrainedmodels`')\n",
    "\n",
    "__all__ = ['inceptionv4', 'inceptionresnetv2', 'nasnetamobile', 'dpn92', 'xception_cadene', 'se_resnet50',\n",
    "           'se_resnet101', 'se_resnext50_32x4d', 'senet154', 'pnasnet5large']\n",
    "\n",
    "def get_model(model_name:str, pretrained:bool, seq:bool=False, pname:str='imagenet', **kwargs):\n",
    "    pretrained = pname if pretrained else None\n",
    "    model = getattr(pretrainedmodels, model_name)(pretrained=pretrained, **kwargs)\n",
    "    return nn.Sequential(*model.children()) if seq else model\n",
    "\n",
    "def inceptionv4(pretrained:bool=False):\n",
    "    model = get_model('inceptionv4', pretrained)\n",
    "    all_layers = list(model.children())\n",
    "    return nn.Sequential(*all_layers[0], *all_layers[1:])\n",
    "model_meta[inceptionv4] = {'cut': -2, 'split': lambda m: (m[0][11], m[1])}\n",
    "\n",
    "def nasnetamobile(pretrained:bool=False):\n",
    "    model = get_model('nasnetamobile', pretrained, num_classes=1000)\n",
    "    model.logits = noop\n",
    "    return nn.Sequential(model)\n",
    "model_meta[nasnetamobile] = {'cut': noop, 'split': lambda m: (list(m[0][0].children())[8], m[1])}\n",
    "\n",
    "def pnasnet5large(pretrained:bool=False):\n",
    "    model = get_model('pnasnet5large', pretrained, num_classes=1000)\n",
    "    model.logits = noop\n",
    "    return nn.Sequential(model)\n",
    "model_meta[pnasnet5large] = {'cut': noop, 'split': lambda m: (list(m[0][0].children())[8], m[1])}\n",
    "\n",
    "def inceptionresnetv2(pretrained:bool=False):   return get_model('inceptionresnetv2', pretrained, seq=True)\n",
    "def dpn92(pretrained:bool=False):               return get_model('dpn92', pretrained, pname='imagenet+5k', seq=True)\n",
    "def xception_cadene(pretrained=False):          return get_model('xception', pretrained, seq=True)\n",
    "def se_resnet50(pretrained:bool=False):         return get_model('se_resnet50', pretrained)\n",
    "def se_resnet101(pretrained:bool=False):        return get_model('se_resnet101', pretrained)\n",
    "def se_resnext50_32x4d(pretrained:bool=False):  return get_model('se_resnext50_32x4d', pretrained)\n",
    "def se_resnext101_32x4d(pretrained:bool=False): return get_model('se_resnext101_32x4d', pretrained)\n",
    "def senet154(pretrained:bool=False):            return get_model('senet154', pretrained)\n",
    "\n",
    "model_meta[inceptionresnetv2] = {'cut': -2, 'split': lambda m: (m[0][9],     m[1])}\n",
    "model_meta[dpn92]             = {'cut': -1, 'split': lambda m: (m[0][0][16], m[1])}\n",
    "model_meta[xception_cadene]   = {'cut': -1, 'split': lambda m: (m[0][11],    m[1])}\n",
    "model_meta[senet154]          = {'cut': -3, 'split': lambda m: (m[0][3],     m[1])}\n",
    "_se_resnet_meta               = {'cut': -2, 'split': lambda m: (m[0][3],     m[1])}\n",
    "model_meta[se_resnet50]         = _se_resnet_meta\n",
    "model_meta[se_resnet101]        = _se_resnet_meta\n",
    "model_meta[se_resnext50_32x4d]  = _se_resnet_meta\n",
    "model_meta[se_resnext101_32x4d] = _se_resnet_meta\n",
    "\n",
    "# TODO: add \"resnext101_32x4d\" \"resnext101_64x4d\" after serialization issue is fixed:\n",
    "# https://github.com/Cadene/pretrained-models.pytorch/pull/128\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
